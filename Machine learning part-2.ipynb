{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfcmxcrqG9NW6i/d2X8F+c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# machine learning part-2"],"metadata":{"id":"T9vkRNFTX_AU"}},{"cell_type":"markdown","source":["**27. What is Simple Linear Regression?**\n","\n","It is a method to predict a dependent variable using a single independent variable with the formula Y = mX + c.\n","\n","**28. What are the key assumptions of Simple Linear Regression?**\n","\n","Linearity, independence, homoscedasticity, normality of errors, and no multicollinearity.\n","\n","**29. What does the coefficient m represent in the equation Y = mX + c?**\n","\n","It represents the slope of the regression line, i.e., the change in Y for a unit change in X.\n","\n","**30. What does the intercept c represent in the equation Y = mX + c?**\n","It is the value of Y when X is zero.\n","\n","**31. How do we calculate the slope m in Simple Linear Regression?**\n","\n","Using the formula: m = covariance(X, Y) / variance(X).\n","\n","**32. What is the purpose of the least squares method in Simple Linear Regression?**\n","\n","To minimize the sum of the squared differences between observed and predicted values.\n","\n","**33. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n","\n","It indicates the proportion of variance in the dependent variable explained by the independent variable(s).\n","\n","**34. What is Multiple Linear Regression?**\n","\n","A regression model with two or more independent variables predicting a dependent variable.\n","\n","**35. What is the main difference between Simple and Multiple Linear Regression?**\n","\n","Simple uses one independent variable; multiple uses more than one.\n","\n","**36. What are the key assumptions of Multiple Linear Regression?**\n","\n","Same as simple regression, plus absence of multicollinearity and linear relationship among all predictors.\n","\n","**37. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n","\n","It means non-constant variance of residuals; it can lead to inefficient estimates and affect hypothesis testing.\n","\n","**38. How can you improve a Multiple Linear Regression model with high multicollinearity?**\n","\n","By removing correlated variables, using PCA, or applying regularization techniques like Ridge or Lasso.\n","\n","**39. What are some common techniques for transforming categorical variables for use in regression models?**\n","\n","One-hot encoding, label encoding, and dummy variable creation.\n","\n","**40. What is the role of interaction terms in Multiple Linear Regression?**\n","\n","They capture the combined effect of two or more variables that is not additive.\n","\n","**41. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n","\n","In multiple regression, the intercept is the expected value of Y when all predictors are 0, which may not be meaningful if 0 isn't a plausible value.\n","\n","**42. What is the significance of the slope in regression analysis, and how does it affect predictions?**\n","\n","It indicates how much the dependent variable changes for a one-unit change in the independent variable.\n","\n","**43. How does the intercept in a regression model provide context for the relationship between variables?**\n","\n","It represents the baseline value of the dependent variable when all predictors are 0.\n","\n","**44. What are the limitations of using R² as a sole measure of model performance?**\n","\n","It doesn't account for model complexity, overfitting, or the actual predictive power.\n","\n","**45. How would you interpret a large standard error for a regression coefficient?**\n","\n","It implies that the coefficient is not estimated precisely and may not be statistically significant.\n","\n","**46. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n","\n","By plotting residuals vs. fitted values; cone-shaped or funnel-like patterns suggest heteroscedasticity.\n","\n","**47. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n","\n","It suggests the model has too many predictors and may be overfitting.\n","\n","\n","**48. Why is it important to scale variables in Multiple Linear Regression?**\n","To ensure all variables contribute equally and prevent bias in models using distance-based metrics.\n","\n","**49. What is polynomial regression?**\n","A form of regression that models the relationship as an nth degree polynomial.\n","\n","**50. How does polynomial regression differ from linear regression?**\n","Polynomial regression can capture non-linear relationships.\n","\n","**51. When is polynomial regression used?**\n","When the data shows a curvilinear trend that cannot be modeled well by a straight line.\n","\n","**52. What is the general equation for polynomial regression?**\n","Y = b0 + b1X + b2X^2 + ... + bnX^n\n","\n","**53. Can polynomial regression be applied to multiple variables?**\n","Yes, it can be extended to multiple variables by adding polynomial terms for each.\n","\n","**54. What are the limitations of polynomial regression?**\n","It can easily overfit, especially with high-degree polynomials, and is sensitive to outliers.\n","\n","**55. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n","Cross-validation, adjusted R², and examining residual plots.\n","\n","**56. Why is visualization important in polynomial regression?**\n","It helps assess the model fit and identify overfitting or underfitting.\n","\n","**57. How is polynomial regression implemented in Python?**\n","Using `PolynomialFeatures` from `sklearn.preprocessing` with a linear model like `LinearRegression`.\n","\n"],"metadata":{"id":"m3XdK7FxYEfn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijeQzSoZX9Gr"},"outputs":[],"source":[]}]}